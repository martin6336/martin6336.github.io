<h1 id="summarization">summarization</h1>

<h2 id="1-abstractive-text-summarization-using-sequence-to-sequence-rnns-and-beyond2016-conll">1. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond(2016 coNLL)</h2>

<h3 id="problems-to-solve">problems to solve</h3>
<ul>
  <li>和MT类似，但是seq2seq的两头没有一一对应关系</li>
  <li>对信息的压缩 in a lossy manner，MT是loss-less</li>
</ul>

<h3 id="contribution">contribution</h3>
<ul>
  <li>attentional encoder-decoder RNN</li>
  <li>propose novel models</li>
  <li>propose a new dataset</li>
</ul>

<h3 id="model-structure">model structure</h3>
<p><strong>1</strong>. Encoder-Decoder RNN with Attention
基本模型依据<a href="https://arxiv.org/pdf/1409.0473v7.pdf" title="Neural machine translation by jointly learning to align and translate">Bahdanau</a>提出的模型。</p>

<ul>
  <li>encoder consists of a bidirectional GRU-RNN</li>
  <li>decoder consists of a uni-directional GRU-RNN($s_{i}=f(s_{i-1}, y_{i-1}, c_{i})$)</li>
  <li>attention mechanism over the source-hidden states</li>
  <li>a soft-max layer over target vocabulary to generate words</li>
  <li>large vocabulary ‘trick’ -LVT(target vocab+source document vocab in a batch)</li>
</ul>

<p><strong>2</strong>. Capturing Keywords using Feature-rich Encoder.
encoder的输入为word embedding+NER tags+TF+IDF+POS</p>

<p><strong>3</strong>. Modeling Rare/Unseen Words using Switching Generator-Pointer
起源于LVT，每次生成计算switch，为1则使用target vocab，为0使用pointer从原文复制单词。switch概率公式如下：
<script type="math/tex">% <![CDATA[
\begin{aligned} P\left(s_{i}=1\right) &=\sigma\left(\mathbf{v}^{s} \cdot\left(\mathbf{W}_{h}^{s} \mathbf{h}_{i}+\mathbf{W}_{e}^{s} \mathbf{E}\left[o_{i-1}\right]\right.\right.\\ &+\mathbf{W}_{c}^{s} \mathbf{c}_{i}+\mathbf{b}^{s} ) ) \end{aligned} %]]></script>
指向各个单词的概率为，i为decoder中位置，j为encoder中位置：
<script type="math/tex">% <![CDATA[
\begin{aligned} P_{i}^{a}(j) & \propto \exp \left(\mathbf{v}^{a} \cdot\left(\mathbf{W}_{h}^{a} \mathbf{h}_{i-1}+\mathbf{W}_{e}^{a} \mathbf{E}\left[o_{i-1}\right]\right.\right.\\ &+\mathbf{W}_{c}^{a} \mathbf{h}_{j}^{d}+\mathbf{b}^{a} ) ) \end{aligned} %]]></script></p>

<p>模型的目标函数为，$g_i$是switch的真实值，只在训练阶段才有：
<script type="math/tex">\begin{array}{l}{\log P(\mathbf{y} | \mathbf{x})=\sum_{i}\left(g_{i} \log \left\{P\left(y_{i} | \mathbf{y}_{-i}, \mathbf{x}\right) P\left(s_{i}\right)\right\}\right.} \\ {+\left(1-g_{i}\right) \log \left\{P\left(p(i) | \mathbf{y}_{-i}, \mathbf{x}\right)\left(1-P\left(s_{i}\right)\right)\right\} )}\end{array}</script>
模型结构如下，计算switch时缺少attention连线
<img src="..\assets\post_image\en_de.png" alt="模型结构" /></p>

<p><strong>4</strong>.  Capturing Hierarchical Document Structure with Hierarchical Attention
计算context vector $c_t$时使用hierarchical attention，对上文的attention进一步处理而已
<script type="math/tex">P^{a}(j)=\frac{P_{w}^{a}(j) P_{s}^{a}(s(j))}{\sum_{k=1}^{N_{d}} P_{w}^{a}(k) P_{s}^{a}(s(k))}</script></p>

<h2 id="2-abstractive-document-summarization-with-a-graph-based-attentional-neural-model2017-acl">2. Abstractive Document Summarization with a Graph-Based Attentional Neural Model(2017 ACL)</h2>

<h3 id="problems-to-solve-1">problems to solve</h3>
<ul>
  <li>key factor of summarization: saliency, fluency, coherence, and novelty</li>
  <li>neural generation are naturally good at fluency, saliency has not been addressed</li>
  <li>generate long sequences; same sentences or phrases are often repeated in the output</li>
</ul>

<h3 id="contribution-1">contribution</h3>
<ul>
  <li>discover the salient information of a document</li>
  <li>a novel graph-based attention mechanism</li>
  <li>a hierarchical decoding algorithm with a reference mechanism</li>
</ul>

<h3 id="model-structure-1">model structure</h3>
<p>The left denotes the traditional Bahdanauet’s attention [$s_{i}=f(s_{i-1}, y_{i-1}, c_{i})$], while the right half denotes the graph-based attention.
<img src="..\assets\post_image\graph-based attention.png" alt="graph-based attention" /></p>

<ol>
  <li>hierarchical encoder-decoder framework</li>
</ol>

<p>$\alpha_i^j$ indicates how much the $i$-th original sentence $s_i$ contributes to generating the $j$-th sentence in summary. $c_j$ is the context vector.
<script type="math/tex">\mathbf{c}_{j}=\sum_{i} \alpha_{i}^{j} \mathbf{h}_{i}</script></p>

<script type="math/tex; mode=display">\alpha_{i}^{j}=\frac{e^{\eta\left(\mathbf{h}_{i}, \mathbf{h}_{j}^{\prime}\right)}}{\sum_{l} e^{\eta\left(\mathbf{h}_{l},\mathbf{h}_{j}^{\prime}\right)}}</script>

<ol>
  <li>
    <p>graph-based attention</p>

    <p>Traditional attention methods not good at judging which sentences are more important to a document. <em>A sentence is important if it’s heavily linked with many important sentences according to Pagerank algorithm.</em> $W(i,j) = h_i^TMh_j$ is the adjacent matrix. $\mathbf{y} \in \mathcal{R}^{n}$ with all elements equal to $\frac{1}{n}$. The importance score of each sentence is :
<script type="math/tex">\mathbf{f}=(1-\lambda)\left(I-\lambda W D^{-1}\right)^{-1} \mathbf{y}</script></p>
  </li>
</ol>

<p>Further, we want compute the rank scores of the original sentences regarding $h_j$. Applying the idea of topic-sensitive pagerank, which alters $\mathbf{y}$.</p>

<script type="math/tex; mode=display">% <![CDATA[
\mathbf{y}_{T}=\left\{\begin{array}{ll}{\frac{1}{|T|}} & {i \in T} \\ {0} & {i \notin T}\end{array}\right. %]]></script>

<p>In order to penalize the model from attending to previously attended sentences. The graph-based attention is finally computed as:</p>

<script type="math/tex; mode=display">\alpha_{i}^{j}=\frac{\max \left(f_{i}^{j}-f_{i}^{j-1}, 0\right)}{\sum_{l}\left(\max \left(f_{l}^{j}-f_{l}^{j-1}, 0\right)\right)}</script>

<ol>
  <li>
    <p>model training
loss function:
<script type="math/tex">\mathcal{L}=\sum_{(Y, X) \in \mathcal{D}}-\log p(Y | X ; \theta)</script></p>
  </li>
  <li>
    <p>hierarchical decoding algorithm</p>

    <p>A beam search strategy may help to alleviate the repetition in a sentence, but the <strong>repetition</strong> in the whole generated summary is remained a problem. Sentence-level beam search is realized by maximizing the accumulated score of all the sentences generated.</p>

    <p>We add an additional term to the score $\tilde{p}\left(y_{\tau}\right)$, ref is a function calculates the ratio of bigram overlap between two texts.
<script type="math/tex">score(\tilde{y_{\tau}}) = \tilde{p}\left(y_{\tau}\right)+
\gamma\left(\operatorname{ref}\left(Y_{\tau-1}+y_{\tau}, s_{*}\right)-\operatorname{ref}\left(Y_{\tau-1}, s_{*}\right)\right)</script></p>
  </li>
</ol>

<h2 id="3-a-deep-reinforced-model-for-abstractive-summarization2018-iclr">3. A DEEP REINFORCED MODEL FOR ABSTRACTIVE SUMMARIZATION(2018 ICLR)</h2>
<h3 id="problem">problem</h3>
<ul>
  <li>repeating phrase problem</li>
  <li>exposure bias</li>
  <li>a large number of summaries are potentially valid, which MLL objective doesn’t take this flexibility into account</li>
</ul>

<h3 id="contribution-2">contribution</h3>

<ul>
  <li>intra-temporal attention in encoder-decoder</li>
  <li>new objective function combining the maximum-likelihood-entropy loss with rl objective</li>
</ul>

<p><img src="..\assets\post_image\deep_reinforce.png" alt="deep_reinforce" /></p>

<h3 id="model-structure-2">model structure</h3>

<p>3 key element: $c_t^e, c_t^d, h_t^d$</p>

<p>$c_t^e$ is the inpute context vectore, $c_t^d$ is the decoder context decoder, $h_t^d$ is the hidden state vector of decoder at time step $t$.</p>

<ul>
  <li>
    <script type="math/tex; mode=display">f\left(h_{t}^{d}, h_{i}^{e}\right)=h_{t}^{d^{T}} W_{\mathrm{attn}}^{e} h_{i}^{e}</script>
  </li>
</ul>

<script type="math/tex; mode=display">e_{t i}=f\left(h_{t}^{d}, h_{i}^{e}\right)</script>

<p>define new temporal attention score, penalizing input tokens that have obtained high attention scores in past attention steps.</p>

<script type="math/tex; mode=display">% <![CDATA[
e_{t i}^{\prime}=\left\{\begin{array}{ll}{\exp \left(e_{t i}\right)} & {\text { if } t=1} \\ {\frac{\exp \left(e_{t i}\right)}{\sum_{j=1}^{t-1} \exp \left(e_{j i}\right)}} & {\text { otherwise }}\end{array}\right. %]]></script>

<script type="math/tex; mode=display">\alpha_{t i}^{e}=\frac{e_{t i}^{\prime}}{\sum_{j=1}^{n} e_{t j}^{\prime}}</script>

<p>obtain final context vector, a weighted sum of all hidden vector in encoder. It differs at different decoding step $t$.</p>

<script type="math/tex; mode=display">c_{t}^{e}=\sum_{i=1}^{n} \alpha_{t i}^{e} h_{i}^{e}</script>

<ul>
  <li>similar as the way we obtain $c_t^e$. To further avoid repeating, we can incorporate more information about the previously decoded sequence into the decoder.</li>
</ul>

<script type="math/tex; mode=display">e_{t t^{\prime}}^{d}=h_{t}^{d^{T}} W_{\mathrm{atn}}^{d} h_{t^{\prime}}^{d}</script>

<script type="math/tex; mode=display">\alpha_{t t^{\prime}}^{d}=\frac{\exp \left(e_{t t^{\prime}}^{d}\right)}{\sum_{j=1}^{t-1} \exp \left(e_{t j}^{d}\right)}</script>

<script type="math/tex; mode=display">c_{t}^{d}=\sum_{j=1}^{t-1} \alpha_{t j}^{d} h_{j}^{d}</script>

<ul>
  <li>adopt switch function $u_t$ in generation process.</li>
</ul>

<script type="math/tex; mode=display">p\left(u_{t}=1\right)=\sigma\left(W_{u}\left[h_{t}^{d}\left\|c_{t}^{e}\right\| c_{t}^{d}\right]+b_{u}\right)</script>

<p>same as pointer network, tradition generation probability is:</p>

<script type="math/tex; mode=display">p\left(y_{t} | u_{t}=0\right)=\operatorname{softmax}\left(W_{\mathrm{out}}\left[h_{t}^{d}\left\|c_{t}^{e}\right\| c_{t}^{d}\right]+b_{\mathrm{out}}\right)</script>

<p>the copy probability is defined as:</p>

<script type="math/tex; mode=display">p\left(y_{t}=x_{i} | u_{t}=1\right)=\alpha_{t i}^{e}</script>

<h3 id="objective">objective</h3>
<p>combine two objective, $\hat{y}$ is the baseline output maximizing the output probability distribution at each time step, $y^s$ is obtained by sampling from the probability distribution at each time step $t$(the whole sentence?), $y^{*}$ is ground truth, $r(y)$ is the reward function,</p>

<p><script type="math/tex">L_{m l}=-\sum_{t=1}^{n^{\prime}} \log p\left(y_{t}^{*} | y_{1}^{*}, \ldots, y_{t-1}^{*}, x\right)</script>
<script type="math/tex">L_{r l}=\left(r(\hat{y})-r\left(y^{s}\right)\right) \sum_{t=1}^{n^{\prime}} \log p\left(y_{t}^{s} | y_{1}^{s}, \ldots, y_{t-1}^{s}, x\right)</script>
<script type="math/tex">L_{\text {mixed}}=\gamma L_{r l}+(1-\gamma) L_{m l}</script></p>

<h3 id="trick">trick</h3>
<ul>
  <li>ground-truth summaries almost never contain the same trigram twice</li>
  <li>introduce some weight-sharing between this embedding matrix and the $W_{out}$ matrix of the token generation layer</li>
</ul>

<h2 id="4-improving-abstraction-in-text-summarization2018-emnlp">4. Improving Abstraction in Text Summarization(2018 EMNLP)</h2>
<h3 id="problem-1">problem</h3>
<ul>
  <li>word overlap metrics do not capture the abstractive nature
    <h3 id="contribution-3">contribution</h3>
  </li>
  <li>decouples the <strong>extraction</strong> and <strong>generation</strong> responsibilities of the decoder by factoring it into a <strong>contextual network</strong> and a <strong>language model</strong>.</li>
  <li>a mixed objective that jointly optimizes the <strong>n-gram overlap</strong> with the ground-truth summary while encouraging <strong>abstraction</strong>.</li>
</ul>

<h3 id="model-structure-3">model structure</h3>

<h4 id="contextual-network-the-same-as-above"><img src="..\assets\post_image\improve_rl.png" alt="improve_rl" />contextual network the same as above</h4>

<h4 id="language-model">language model</h4>

<p>The input of language model is the same as decoder at each time step $t$. Combing $h^{lm}<em>t$ with $[h</em>{t}^{d}|c_{t}^{e}| c_{t}^{d}]$, we have $h^{fuse}_t$
<script type="math/tex">% <![CDATA[
\begin{aligned} f_{t} &=\operatorname{sigmoid}\left(W^{\operatorname{lm}}\left[r_{t} ; h_{3, t}^{\operatorname{lm}}\right]+b^{\operatorname{lm}}\right) \\ g_{t} &=W^{\mathrm{fuse}}\left(\left[r_{t} ; g_{t} \odot h_{3, t}^{\operatorname{lm}}\right]\right)+b^{\mathrm{fuse}} \\ h_{t}^{\mathrm{fuse}} &=\operatorname{ReLU}\left(g_{t}\right) \end{aligned} %]]></script></p>

<h4 id="abstractive-reward">abstractive reward</h4>

<p>changing the the reward function  $r(y)$.</p>

<h2 id="5-summarunner2017-aaai">5. SummaRuNNer(2017 AAAI)</h2>

<h3 id="problem-2">problem</h3>

<h3 id="contribution-4">contribution</h3>

<h3 id="model-structure-4">model structure</h3>

<p><img src="..\assets\post_image\SummaRuNNer.png" alt="SummaRuNNer" /></p>

<h3 id="extractive-training">extractive training</h3>

<p>The first layer of the RNN runs at the word level, second layer of bi-directional RNN that runs at the sentence-level. Using average pooling to get sentence or document representation.
<script type="math/tex">\mathbf{d}=\tanh \left(W_{d} \frac{1}{N_{d}} \sum_{j=1}^{N^{d}}\left[\mathbf{h}_{j}^{f}, \mathbf{h}_{j}^{b}\right]+\mathbf{b}\right)</script></p>

<p>The binary decision is defined as below. Each row represents content, salience, novelty, absolute and relative positional embedding respectively.
<script type="math/tex">\begin{array}{r}{P\left(y_{j}=1 | \mathbf{h}_{j}, \mathbf{s}_{j}, \mathbf{d}\right)=\sigma\left(W_{c} \mathbf{h}_{j}\right.} \\ {+\mathbf{h}_{j}^{T} W_{s} \mathbf{d}} \\ {-\mathbf{h}_{j}^{T} W_{r} \tanh \left(\mathbf{s}_{\mathbf{j}}\right)} \\ {+W_{a p} \mathbf{p}_{j}^{r}} \\ {+W_{r p} \mathbf{p}_{j}^{r}} \\ {+b )}\end{array}</script>
<script type="math/tex">\mathbf{s}_{j}=\sum_{i=1}^{j-1} \mathbf{h}_{i} P\left(y_{i}=1 | \mathbf{h}_{i}, \mathbf{s}_{i}, \mathbf{d}\right)</script></p>

<p>The objective is to minimize the negative log-likelihood of the observed labels at training time:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} l(\mathbf{W}, \mathbf{b}) &=-\sum_{d=1}^{N} \sum_{j=1}^{N_{d}}\left(y_{j}^{d} \log P\left(y_{j}^{d}=1 | \mathbf{h}_{j}^{d}, \mathbf{s}_{j}^{d}, \mathbf{d}_{d}\right)\right.\\ &+\left(1-y_{j}^{d}\right) \log \left(1-P\left(y_{j}^{d}=1 | \mathbf{h}_{j}^{d}, \mathbf{s}_{j}^{d}, \mathbf{d}_{d}\right)\right) \end{aligned} %]]></script>

<h3 id="how-to-get-extractive-label">how to get extractive label</h3>

<p>We employ a greedy approach, where we add one sentence at a time incrementally to the summary, such that the Rouge score of the current set of selected sentences is maximized with respect to the entire summary.</p>

<h3 id="abstractive-training">abstractive training</h3>

<p>We need to label sentences in the document for extractive training, which will bring loss. This paper propose a novel training technique to train SummaRuNNer abstractively. We <strong>couple the initial model with a RNN decoder</strong> that models the generation of abstractive summaries at training time only. We modify the first three equations of GRU based RNN, $s_{-1}$ is the summary representation computed at the last sentence of the sentence-level bidirectional RNN. It’s the only bridge between the initial model and the coupled decoder.</p>

<p><script type="math/tex">% <![CDATA[
\begin{aligned} \mathbf{u}_{j} &=\sigma\left(\mathbf{W}_{u x} \mathbf{x}_{j}+\mathbf{W}_{u h} \mathbf{h}_{j-1}+\mathbf{b}_{u}\right) \\ \mathbf{r}_{j} &=\sigma\left(\mathbf{W}_{r x} \mathbf{x}_{j}+\mathbf{W}_{r h} \mathbf{h}_{j-1}+\mathbf{b}_{r}\right) \\ \mathbf{h}_{j}^{\prime} &=\tanh \left(\mathbf{W}_{h x} \mathbf{x}_{j}+\mathbf{W}_{h h}\left(\mathbf{r}_{j} \odot \mathbf{h}_{j-1}\right)+\mathbf{b}_{h}\right) \\ \mathbf{h}_{j} &=\left(1-\mathbf{u}_{j}\right) \odot \mathbf{h}_{j}^{\prime}+\mathbf{u}_{j} \odot \mathbf{h}_{j-1} \end{aligned} %]]></script>
<script type="math/tex">% <![CDATA[
\begin{array}{ll}{\mathbf{u}_{k}=} & {\sigma\left(\mathbf{W}_{u x}^{\prime} \mathbf{x}_{k}+\mathbf{W}_{u h}^{\prime} \mathbf{h}_{k-1}+\mathbf{W}_{u c}^{\prime} \mathbf{s}_{-1}+\mathbf{b}_{u}^{\prime}\right)} \\ {\mathbf{r}_{k}=} & {\sigma\left(\mathbf{W}^{\prime} r x_{k}+\mathbf{W}_{r h}^{\prime} \mathbf{h}_{k-1}+\mathbf{W}_{r c}^{\prime} \mathbf{s}_{-1}+\mathbf{b}_{r}^{\prime}\right)} \\ {\mathbf{h}_{k}^{\prime}=} & {\tanh \left(\mathbf{W}_{h x}^{\prime} \mathbf{x}_{k}+\mathbf{W}_{h h}^{\prime}\left(\mathbf{r}_{k} \odot \mathbf{h}_{k-1}\right)+\right.}{\mathbf{W}_{h c}^{\prime} \mathbf{s}_{-1}+\mathbf{b}_{h}^{\prime} )}\end{array} %]]></script></p>

<p>The emission at each time-step is determined by a feed-forward layer $f$ followed by a softmax layer that assigns $\mathbf{P}_{k}$</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned} \mathbf{f}_{k} &=\tanh \left(\mathbf{W}_{f h}^{\prime} \mathbf{h}_{k}+\mathbf{W}_{f x}^{\prime} \mathbf{x}_{k}+\mathbf{W}^{\prime} f_{c} \mathbf{s}_{-1}+\mathbf{b}_{f}^{\prime}\right) \\ \mathbf{P}_{\mathbf{v}}(\mathbf{w})_{k} &=\operatorname{softmax}\left(\mathbf{W}_{v}^{\prime} \mathbf{f}_{k}+\mathbf{b}_{v}^{\prime}\right) \end{aligned} %]]></script>

<p>we minimize the negative log-likelihood of the words in the reference summary as follows.
<script type="math/tex">l\left(\mathbf{W}, \mathbf{b}, \mathbf{W}^{\prime}, \mathbf{b}^{\prime}\right)=-\sum_{k=1}^{N_{s}} \log \left(\mathbf{P}_{\mathbf{v}}\left(w_{k}\right)\right)</script>
where $N_s$ is the number of words in the reference summary.</p>

